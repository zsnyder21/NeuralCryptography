# Neural Cryptography

## Motivation
Encryption is a necessity. It protects private/sensitive information or enhances the security of communication between
client applications and servers. There are many different forms of encryption, but fundamentally the idea is to make
sensitive data unintelligible and unusable to an unauthorized viewer. For example, someone gains access to a SQL
database containing user logins and passwords. If the passwords are not encrypted, gaining access to any user's account
just became trivial. If the passwords are encrypted, however, they would still need to be decrypted to gain
access - no easy task!

Another application of encryption  is sending covert information or secret messages. Think the Enigma Game - breaking
the German code was instrumental in the overthrow of the Nazi regime.

Or suppose information needs to be sent to a covert operative - would it be sent in plaintext? Of course not! It would
be encoded in some way. Another level up would be to send an encrypted message to this operative in such a way that an
outside observer wouldn't even be aware that there is a covert message being sent. To achieve this goal, we can embed a
message within an image.

With that in mind, our goal will be to construct a convolutional neural network to encrypt and decrypt information within
an image.

## The Network
### Overview
The idea here is to create a network that encrypts information by combining text with an image and can then separate the
text from the new image to recover the text. Note that all of the text must be tokenized in order for the network to 
properly process it.

<img src="./img/Diagrams/Workflow.png">

The CNN is divided into two parts:
* **Encoder**: This portion of the network takes an image and text and embeds the text within the image
  

* **Decoder**: This segment of the network takes the output of the encoder and extracts the text from it

### Training the Network
To train the network, we use randomly generated images and strings. This helps the network to be robust and work with
a very wide variety of text and images. Examples of random images and text can be seen below

|             Random Image 1              |               Random Image 2             |
|------------------------------|-----------------------------------------------------|
|<img src="./img/Raw/Random1_NoAxes.png"> | <img src="./img/Raw/Random2_NoAxes.png"> |


Note that some characters do not display very well within typical text editors or consoles.
* Random Text 1: +|lIgtNF:'	uNT'tX1lB!Tah9]QV;=Xsfi6Cs#,_#.|#wSW (p"EpY&
G#*L-!vhtj%P[k{O8v`


* Random Text 2: pQ9u>q,zH u77">uTRT#)wFwvw|44&^AWb=&HU#r*ZZ5yWI{^/5.sU},hK2n.7Z
,Q}<w527/&[\="6$&M
  
The general structure of the network is outlined in the following diagram:

<img src="./img/Diagrams/NeuralNetStructure.png">

I trained the network for a variable number of epochs. Sentence reconstruction accuracy quickly plateaued at 100% after
the 2nd epoch. Using a custom threshold Callback in the model, I stopped training when image reconstruction loss reached
0.008. This threshold value was chosen as a trade-off between time and to prevent overfitting.

Final model scores were as follows:

* Sentence Reconstruction Loss: 2.7585e-4


* Image Reconstruction Loss: 0.0078


* Sentence Reconstruction Categorical Accuracy: 100%

## Results
Now that we have a well-trained model, let's try and apply it to something. Take for example this Kip Thorne quote: "A
black hole really is an object with very rich structure, just like Earth has a rich structure of mountains, valleys,
oceans, and so forth. Its warped space whirls around the central singularity like air in a tornado."

We will embed this quote within the following image:

<img src="img/Raw/twister.png">

A little bit of pre-processing is necessary to prepare the image for the network. This pre-processing simply ensures
that the image has the dimensions that the neural network expects: (2000, 2000, 3). That is, a 2000x2000 RGB image.
After pre-processing our image looks like this:

<img src="img/PreProcessed/twister.png">

The filler bar color is chosen to be the average color of the image, so as to not ruin the aesthetics. Note that if the
image were larger, the image would be naively cropped down to size, taking the first 2000 pixels in the appropriate
dimensions.


After feeding this pre-processed image and our chosen text, the network outputs the following image:

<img src="img/Embedded/twister.png">

Those images look nearly identical to me. To put that into quantitative terms, the mean pixel difference is ~0.0071,
nearly imperceptible.


All that's left is to take this image with embedded text and attempt to extract the text from it. Feeding the previous
image into the decoder segment of the network, we obtain the following text:

"A black hole really is an object with very rich structure, just like Earth has a rich structure of mountains, valleys,
oceans, and so forth. Its warped space whirls around the central singularity like air in a tornado."

This is exactly the text we input into the network, demonstrating that we can effectively encode text into an image,
nearly imperceptibly, and with extremely high accuracy decode the text from the image.


## Image Corruption
Say that we have encoded a message of length much smaller than the image size (sentence length around 10% of image size)
and have sent it overseas for someone to decode. Unfortunately, while in transit some of the information has been lost or
corrupted. I've simulated this by replacing random percentages of the pixels within the image with either black, white, 
or random pixels. How well can we reconstruct the message now?

The corrupted images (with 30% of the pixels corrupted) appear as follows: 

|             Corrupted Black              |               Corrupted White           |         Corrupted Random  |
|------------------------------|-----------------------------------------------------|---------------------------------|
|<img src="./img/Corrupted/twisterBlack.png"> | <img src="./img/Corrupted/twisterWhite.png"> | <img src="./img/Corrupted/twisterRandom.png">|

These images are decoded as follows:

|Corruption  Method |            Decoded Sentence                         |
|:-------------------------:|:-----------------------------|
|  **Black-Pixel Corrupted**             |           A black hole really is an object with very rich structure, just likeÄ¶ Earth has a rich structure of mountains, valÄ¶leeyeÌŠs, oceans, and so forth. Its warped space whirs around the central singularÄ¶ity like air in a tornado.              |
| **White-Pixel Corrupted** |AÃ—blÅ¡Å¦WBacÂ”k hole reË‹Ë®ÄšÅµaÇ²Ã½llyÄ‡Éš isÇ©aÆšË•n objÄ—eÆƒct wÉ´Æ¯ÆŸÌitÃ”h veÃ„ÊyJ Ë”Æ®ricÆ¤ sÅƒÆ•È…tÅ®ructureÊ¹, Â›Ä¤ÂƒÅ±Å½ust{ E{Ä˜lÄ¨<e Earth hÈ„s aÈ¡Ä” rÉµich Ç¯strucË©Â•ure Ã´oÃ†É¼f)ÃˆÄ‘ mountÆ¹aÉ„Å¼ns, valleys,É±Ã¬ oceeÂ»yeaens, anÅ¶dÂÅµ soÇ fortÄ“Ê±h.1Ç‡ 4Å¹Ë›tMË¤Ä¾Ã¿ÂÃ‚Ê¤Å¶?ÅºÃÊˆÃ®s&#124;Ç§ Ä¶Ì4ÈaÊÄšÄ‘Â‘.Ç½Ê¿hÄ¢È•É‘eÄ½dQÈ› Â¼Â˜Ì‡È—Ã³pÄÈºÂ¸aÂ®ÊÄ^Ä‹Â·Ä²`ÃŒÊ®Ë­ÊƒÆ¡Äe 7Å¸Ì”Ê´Â¾ÃºÈ¦Å¼ÌÃ­È®ÂŒÂ‰Â”È®ËÌ›É«:Â¾Ã³ÌšÈ‘iÄÉ¼ÊœÂ‘Ç”Ë”É½ÅˆÈ¯Ê±Ë”Â‘Ã‹È¥4Ä¶Å¾Å¿Æ©Æ¥É±Ä§È¶É›É¶É™+Ë„sÈ“É„Mr&#124;È‰Ä¿ÌšFLrÇ‘Å¤zÃ§suÄ•ÃŒnÆ¶Ì‚Ê‚ÂÇ¨Ç™,Æ¶sÄ²Â¿OÆ´Â…Ã‘tË¡Â¸Ë¬hÂ‡Ç®UÂ½ ËºÆ—Ì’Ã¼ÄŸTÂ›ÈªÅˆÄ´ÇŠËÂ’Æ‡Ë”ÇÃ›Ë½Ç…ÇƒÅ›È•Ë¯Ä¾É¶ÃÅ“ËËƒÊ¾ÅµÅ¨Ë¥{KÆ´aÄ¶Ë­Èµ) ÆÃ­È¨È²Ä¾ÌˆnÇÈ¿ÇŸ3ÄÃ¿mpgÈ©Ê¸Ç‘ÌÅ§mÇ‚uÈ†ËŸlaritÅË€Ìš Ã lÄ„Ç™iÅ‘ÃkeÇ­ air in ÌŠÃ¨aÆƒtoËÅ·rn.adÅ›o.|
| **Random-Pixel Corrupted**  | A black hole really is an object with very rich structure, just like Earth has a ricÆ‘h structÆ“ure of \`mountainsee, valleys, oceans, and so fozrthÄµ. IÌˆ\`Ë”ts ËˆwaË”Ë­Å±È„zËƒÇŠÂÈ„zË­nÉ¶È–Ä’rpÂ‘eÈ•d Ë”zÈ’sÊ©Ä¾paÉ–cË”Ë”eÉ™È„ wÄŒhirÃ¿ÄŒlË­È»sÌÈ•8gÇ‡arË¯Â·ouË”ndÉ›Å›ÇÈ„thÈ„eÉ½ÌÇŠÄÃ¥Ë” centra-Ä¶lÄŒË” singularity like air in a tornado.|


We can no longer extract the exact sentence anymore. Though it isn't identical, the essence of the message is still
there for black-pixel corrupted images. These results are unique to black pixels: A black pixel decodes to a null
character, and so this doesn't interfere with our decrypted sentence much. If we use different colored pixels to corrupt
our image, however, the decoded messages deviate from readable levels worse, as we can see above with the
white-pixel and random-pixel corrupted images.


To quantify these deviations from readable levels, the Levenshtein distance (computed by counting the number of
replacements, insertions, or deletions it takes to get from one sentence to the other) and ratio can be used. Below I've
plotted this for various corruption percentages

|             Corrupted Black              |               Corrupted White           |         Corrupted Random  |
|------------------------------|-----------------------------------------------------|---------------------------------|
|<img src="./img/Plots/LevenshteinDistanceBlack.png"> | <img src="./img/Plots/LevenshteinDistanceWhite.png"> | <img src="./img/Plots/LevenshteinDistanceRandom.png">|
|<img src="./img/Plots/LevenshteinRatioBlack.png"> | <img src="./img/Plots/LevenshteinRatioWhite.png"> | <img src="./img/Plots/LevenshteinRatioRandom.png">|

Below we see plots of both Levenshtein distance and ratio as a function of image corruption. We can see that the ratio
starts to sharply descend as we near 40% image corruption, suggesting that images corrupted beyond this point may not be
able to have text properly decoded via this neural net. The plot of Levenshtein distance also supports this tipping
point. 


È„È„È„È„È„Ä¶Ä¶È„È„Ä¶È„zÈ„È„Ä¶È„Ë”Ä¶Ä¶Ä¶ÄŒÄ¶È„È„ÄŒÄ¶Ä¶Ä¶È„Ä¶ÄŒÄ¶Ä¶Ë”È„Ä¶ÄŒË”È„Ë”Ä¶ÄŒÄ¶È„È„È„È„Ä¶Ë”È„È„È„Ë”Ä¶Ä¶ÄŒÄ¶È„Ä¶ÄŒÄ¶È„È„Ä¶È„È„Ä¶Ä¶È„ÄŒË”Ä¶Ä¶È„Ä¶Ä¶Ä¶Ä¶È„Ä¶È„Ä¶Ë”Ä¶Ä¶È„Ä¶È„Ë”Ä¶È„È„È„Ä¶È„ÄŒÄ¶Ä¶È„È„È„Ä¶È„È„Ä¶Ä¶È„Ä¶Ä¶Ä¶È„Ë”Ä¶Ë”È„Ä¶Ä¶Ä¶È„È„ÄŒÈ„Ä¶È„È„Ä¶Ä¶È„Ä¶Ä¶Ä¶Ä¶Ä¶ÄŒÄŒÈ„È„È„Ë”ÄŒÈ„È„Ä¶Ä¶È„È„ÄŒÈ„È„È„Ä¶È„Ä¶È„Ä¶È„È„ÄŒÄ¶rÈ„Ä¶Ä¶Ë”Ä¶ÄŒÈ„Ä¶È„Ä¶Ä¶ÄŒÄ¶ÄŒÄ¶ÄŒÈ„Ä¶Ä¶Ë”È„È„È„È„Ä¶È„È„ÄŒÈ„Ä¶Ä¶Ä¶ÄŒÄ¶È„Ë”Ë”ÄŒÈ„È„È„È„È„Ä¶È„Ä¶ÄŒÈ„rÄ¶Ä¶Ä¶ÄŒÄ¶Ë”Ä¶È„È„Â‘Ä¶Â‘Ä¶È„È„È„Ä¶È„ÄŒÉ–Ä¶Ä¶,Ä¶Ä¶Ä¶Ä¶Ë”È„È„Ë”È„È„Ä¶Ä¶È„Ä¶Ë”Ë­Ë”È„È„ÄŒÄ¶Ë”È„zÄ¶Ä¶È„È„Ä¶Ä¶È„ÄŒÄŒÈ„Ä¶È„Ä¶ÄŒÄ¶È„Â‚Ë”È„È„`È„Ë”Å±ÄŒË”Ä¶`È¶É½Ä¶ÄŒÈ„Ä¶Ä¶È„Ë”È„Ä¶Ä¶ÄŒÄ¶ÄŒË”ÄŒzzÄ¶È„`É¼Ä¶È„È„È„Ë”È„È„ÅˆÄ¶Ä¶È„Ä¶È„Ä¶Ä¶Ë”È„Ë”Ä¶Ä¶Ë”È„Ä¶Ä¶È„Ä¶Ä¶zÈ„Ä¶Ä¶Ä¶È„Ä¶Ä¶È„È„È„Ä¶È„Ä¶È„Ä¶ÄŒÈ„rÈ„È„È„Ä¶Ä¶Ä¶Ä¶Ä¶Ä¶zÈ„È„Ä¶È„È„Ä¶zÂ·Ä¶È„Ä¶Ä¶È„Ä¶Ä¶Ä¶È„Ë”Ä¶Ë”Ä¶Ë”Ä¶Ä¶Å‹È„hÄ¶È„Ä¶Ë”È„È„Ä¶È„ÄŒÂzÄ¶È„È„É–Ä¶Æ‡Ä¶Å‹È„Ä¶ÄŒÄ¶Ä¶È„É¼Ä¶Ç‡Ä¶È„Ä¶È„Ë¯Ä¶È„Ä¶É–Ä¶È„È„Æ¢Ä¶Ä¶È„È„È„`Ä¶Â·È„Ä¶È„Ä¶È„Ä¶`Ä¶Ë”È„Ä¶Ä¶Ä¶Ä¶Ä¶Â‘Ä¶È„Ä¶ÄŒÈ„È„ÄŒÈ„Ë”È„`Ä¶È„Ë”È„Ä¶Ä¶Ä¶Ä¶`Ä¶ÄŒÄ¶Ä¶È„Ä¶Ä¶Ä¶Ä¶Ä¶È„ÄŒÈ„Ë”,È„ÄŒzÄ¶Ë”È„Ä¶Ä¶É–ÄŒË”Ä¶Ä¶Ë”Ä¶Ä¶Ä¶Ä¶Ä¶Å‹Ä¶Ä¶Ë”Â·È„Ë”È¶Ä¶Â‘Ë”ÄŒË¢Å¨`Â‘eÉ–Ê©Ä¶É¶Ë”Ä¶rÂÂ¼Ë”É–Ä¶Ë”É–Ä¶Â¸É–ÇŠÈ„Ä¶É¶zË­Ä¶`Ä¶È„È„Æ `Ä¶zÈ„Ä¶É½Ä¶È„ÄŒÄ¶Ä¶Ä¶Ä¶È„Ë”Ä¶ÄŒÈ„È„ÄŒÈ„È„È„È„È„Ä¶Ä¶Ä¶ÄŒÄŒzÄ¶nÈ„Ä¶È„Ä¶Ä¶È„Ä¶Ä¶Ä¶Ä¶ÄŒÈ„Ë”Ë”È„Ä¶Ä¶Ä¶Ä¶ÄŒË”Ä¶Ä¶Ä¶Ä¶È„ÄŒÄ¶È„È„Ä¶fÄ¶Ä¶ÄŒÈ„Ä¶È„Ä¶È„È„rÈ„Ä¶È„È„È„Ä¶Ä¶Ä¶Ä¶Ä¶È„Ä¶Ä¶Ä¶ÄŒË”Ä¶Ä¶È„ÄŒÈ„È„ÄŒÈ„Ä¶Ä¶È„È„Ä¶È„È„Ä¶Ä¶È„È„È„Ä¶Ä¶È„Ä¶Ä¶Ä¶È„Ä¶È„Ä¶È„Ä¶Ä¶È„Ä¶È„Ä¶È„Ä¶Ä¶Ä¶È„Ä¶È„Ä¶Ä¶È„È„Ä¶Ä¶Ä¶Ä¶È„Ä¶È„È„È„È„È„È„È„È„Ä¶È„Ä¶Ä¶È„Ä¶È„Ä¶È„È„È„rÄ¶Ä¶È„È„È„Ä¶È„È„Ä¶È„Ä¶Ë”Ä¶Ä¶È„Ä¶È„ÄŒÄ¶Ä¶È„È„ÄŒÈ„È„È„È„Ä¶Ä¶Ä¶È„È„Ä¶ÄŒÈ„È„Ä¶È„È„È„È„È„Ä¶Ä¶Ä¶ÄŒÄ¶È„È„È„È„È„Ë”Ä¶È„Ä¶È„Ä¶È„È„Ä¶É–Ä¶È„È„Ä¶Ä¶È„È„Ä¶ÄŒÈ„È„È„È„È„È„È„Ë”ÄŒÄ¶Ä¶Ä¶Ä¶Ä¶È„Ä¶ÄŒÄ¶Ä¶Ä¶rÈ„Ä¶ÄŒÄ¶Ä¶ÄŒÄ¶È„È„Ä¶Ä¶ÄŒÄ¶È„È„È„Ä¶È„È„È„Ä¶È„Ä¶È„Ä¶ÄŒÈ„ÄŒÈ„È„